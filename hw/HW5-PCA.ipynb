{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style=\"text-align: right;\"> &#9989; Put your name here</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to successfully complete this assignment, you must follow all instructions in this notebook and upload your edited ipynb file with your answers on or before **11:59pm on Friday Apr. 5th**.\n",
    "\n",
    "**BIG HINT:** Read the entire homework before starting.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Assignment: Principal Component Analysis\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/lNHqt.gif\">\n",
    "From: https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline for Homework 5\n",
    "\n",
    "</p>\n",
    "\n",
    "1. Utah Teapot Example\n",
    "1. PCA in High Dimensional Space (Images as Vectors)\n",
    "1. Picking the \"best\" Feature Vectors\n",
    "1. Using PCA for Feature Selection in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# 1. Utah Teapot Example \n",
    "\n",
    "**NOTE:** The following code may not work on your computer. \n",
    "If there is a problem, work with your classmates.\n",
    "\n",
    "https://sketchfab.com/models/1092c2832df14099807f66c8b792374d\n",
    "\n",
    "&#9989;  <font color=red>** DO THIS:**</font> Pretend you own a teapot and you want to sell it on ebay. \n",
    "However, you only get to submit one picture. \n",
    "Use your mouse to rotate the teapot in the figure below to provide the \"best\" picture of the teapot. \n",
    "I.e. given a single 2D projection of the 3D object, what is the best angle to take a picture that best shows the shape of the teapot? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the associated webpage in a new window\n",
    "import IPython\n",
    "url = 'some url....'\n",
    "iframe = '''<div class=\"sketchfab-embed-wrapper\"><iframe width=\"640\" height=\"480\" src=\"https://sketchfab.com/models/1092c2832df14099807f66c8b792374d/embed\" frameborder=\"0\" allowvr allowfullscreen mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\" onmousewheel=\"\"></iframe>\n",
    "\n",
    "<p style=\"font-size: 13px; font-weight: normal; margin: 5px; color: #4A4A4A;\">\n",
    "    <a href=\"https://sketchfab.com/models/1092c2832df14099807f66c8b792374d?utm_medium=embed&utm_source=website&utm_campain=share-popup\" target=\"_blank\" style=\"font-weight: bold; color: #1CAAD9;\">The Utah Teapot</a>\n",
    "    by <a href=\"https://sketchfab.com/3dgraphics?utm_medium=embed&utm_source=website&utm_campain=share-popup\" target=\"_blank\" style=\"font-weight: bold; color: #1CAAD9;\">3D graphics 101</a>\n",
    "    on <a href=\"https://sketchfab.com?utm_medium=embed&utm_source=website&utm_campain=share-popup\" target=\"_blank\" style=\"font-weight: bold; color: #1CAAD9;\">Sketchfab</a>\n",
    "</p>\n",
    "</div>'''\n",
    "IPython.display.HTML(iframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code downloads a \"point cloud\" version of the Utah teapot. \n",
    "Read more about this model from the following website: http://www.holmes3d.net/graphics/teapot/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen, urlretrieve\n",
    "\n",
    "url = 'http://www.holmes3d.net/graphics/teapot/teapotCGA.bpt'\n",
    "file = 'teapotCGA.bpt'\n",
    "\n",
    "urlretrieve(url, file);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on the following link to see the contents of the ```bpt``` file. \n",
    "Note, most of the file contains 3d points in $(x,y,z)$ coordinates.  \n",
    "\n",
    "http://www.holmes3d.net/graphics/teapot/teapotCGA.bpt\n",
    "\n",
    "The rest of the file contains information about how the points are connected to generate a triagular mesh. \n",
    "For this assignment, let's just look at the points. \n",
    "The following code opens the file and reads any line with 3 values as an $(x,y,z)$ point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the file\n",
    "\n",
    "import sympy as sym\n",
    "import numpy as np\n",
    "sym.init_printing()\n",
    "\n",
    "# open the file for reading\n",
    "filehandle = open(file, 'r')  \n",
    "points = []\n",
    "while True:  \n",
    "    # read a single line\n",
    "    line = filehandle.readline()\n",
    "    if not line:\n",
    "        break\n",
    "    coords = line.split()\n",
    "    if len(coords) == 3:\n",
    "        point = []\n",
    "        for c in coords:\n",
    "            c = float(c)\n",
    "            point.append(c)\n",
    "        points.append(point)\n",
    "\n",
    "# close the pointer to that file\n",
    "filehandle.close()  \n",
    "\n",
    "#convert the points to a numpy array\n",
    "points = np.array(points).T\n",
    "\n",
    "P = np.matrix(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Plot the 3D points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the points in 3D \n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(points[0,:], points[1,:], points[2,:])\n",
    "\n",
    "ax.set_xlabel('X');\n",
    "ax.set_ylabel('Y');\n",
    "ax.set_zlabel('Z');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989;  <font color=red>** DO THIS:**</font> Plot the points in 2D. \n",
    "The following is a simple projection onto the $(x,y)$ plane. \n",
    "Change the numbers in the following code to view the plot from other angles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(points[0,:],points[1,:]);\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually Calculate PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the principal components using the following steps. Recall in the first midterm exam, we started a simple version of PCA.\n",
    "\n",
    "### Step 1: Center the data\n",
    "Move all 512 points in $P$ such that the average points is the original (0,0,0) in 3d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#####Start your code here #####\n",
    "A = \n",
    "print(np.allclose(A.mean(axis=1) ,0) )\n",
    "plt.scatter(np.array(A[0,:]),np.array(A[1,:]));\n",
    "plt.axis('equal');\n",
    "#####End of your code here#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create the covariance matrix $AA^\\top$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989;  <font color=red>** DO THIS:**</font> Calculate the Covariance matrix $C$ from $A$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Start your code here #####\n",
    "C = \n",
    "#####End of your code here#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Calculate the eigenvalues and eigenvectors of $C$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989;  <font color=red>** DO THIS:**</font> Calculate the eigenvalues and eigenvectors of $C$. Call the variables ```vals``` and ```vecs```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Start your code here #####\n",
    "vals, vecs = \n",
    "#####End of your code here#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Sort the eigenvectors in terms of eigenvalues (from biggest to smallest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989;  <font color=red>** DO THIS:**</font> Sort the eigenvectors by their eigenvalues (biggest to smallest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Start your code here #####\n",
    "\n",
    "\n",
    "#####End of your code here#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Transform original points into the basis coordinates.  \n",
    "Remember that the eigenvectors form a basis and the vectors returned by ```numpy``` are already normalized. \n",
    "In fact the vectors are already returned as a matrix such that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = vecs\n",
    "sym.Matrix(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the basis transformation matrix does the following conversion:\n",
    "\n",
    "$$P = BP_b$$\n",
    "\n",
    "We want to calculate $P_b$ under the new basis, so we need to invert $B$:\n",
    "\n",
    "$$P_b = B^{-1}P$$\n",
    "\n",
    "NOTE: Since our matrix $C$ is symmetric, the basis vectors $B$ constructed from eigenvectors are orthonormal. So the following is also true:\n",
    "\n",
    "$$P_b = B^\\top P$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or. in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Start your code here #####\n",
    "Pw =\n",
    "#####End of your code here#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Visualize the final results\n",
    "\n",
    "Plot the first and second dimensions of the newly transferred points. \n",
    "This should be our first and second basis vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(np.array(Pw[0,:]),np.array(Pw[1,:]));\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do it again... but faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Steps 1-4\n",
    "pca = PCA()\n",
    "pca.fit(P.T)\n",
    "\n",
    "#Step 5\n",
    "pca_points = pca.transform(P.T)\n",
    "\n",
    "#Step 6\n",
    "plt.scatter(pca_points[:,0],pca_points[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is the above image upside down? Let's look at the two transformation matrices?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vecs.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989;  <font color=red>** QUESTION:**</font> What is different between these two transforms? Do these differences matter? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your answer to the above question here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. PCA in High Dimensional Space (Images as Vectors)\n",
    "\n",
    "\n",
    "The faces in the wild dataset is often used to test face recognition algorithms. \n",
    "The Sklearn library comes with a function to download the dataset. \n",
    "\n",
    "\n",
    "&#9989;  <font color=red>** DO THIS:**</font> Run the following code to download the faces in the wild dataset.\n",
    "\n",
    "\n",
    "**NOTE** This may take some time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the data from scikit-learn\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_lfw_people, load_digits\n",
    "\n",
    "sk_data = fetch_lfw_people(min_faces_per_person=50, resize=0.4)\n",
    "\n",
    "feature_vectors = sk_data.data\n",
    "class_labels = sk_data.target\n",
    "categories = sk_data.target_names\n",
    "n_samples, n_features = feature_vectors.shape\n",
    "N, h, w = sk_data.images.shape\n",
    "n_classes = len(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```sk_data``` structure has a couple of interesting components:\n",
    "\n",
    "- data - Raw image data (stored as vectors)\n",
    "- target - name of the person in the image (stored as a number)\n",
    "- categories - lookup table for people's name (number to name)\n",
    "\n",
    "Use the following to view the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "from ipywidgets import interact\n",
    "\n",
    "def browse_images(images, labels, categories):\n",
    "    n = len(images)\n",
    "    def view_image(i):\n",
    "        plt.imshow(images[i], cmap=plt.cm.gray, interpolation='nearest')\n",
    "        plt.title('%s' % categories[labels[i]])\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    interact(view_image, i=(0,n-1))\n",
    "browse_images(sk_data.images, sk_data.target, sk_data.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the ```feature_vector``` matrix. \n",
    "Each row of the matrix represents a different person (so there are 1560 people in the dataset) and each column represents a different feature (in this case, there are $h\\times w$ features or one feature for each pixel). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('rows=',h,'columns=',w,'pixels=',h*w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our ```numpy``` matrix $A$ to just be the ```feature_vectors```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.matrix(feature_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Algorithm\n",
    "\n",
    "- STEP 1: Center the Data\n",
    "- STEP 2: Create the covariance matrix $AA^\\top$\n",
    "- STEP 3: Calculate the eigenvalues and eigenvectors of $C$\n",
    "- STEP 4: Sort the eigenvectors in terms of eigenvalues (from biggest to smallest)\n",
    "- STEP 5: Transform original points into the basis coordinates.  \n",
    "- STEP 6: Visualize the final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# STEP !: Center the Data \n",
    "mean_face = A.mean(axis=0)\n",
    "\n",
    "A2 = A - mean_face\n",
    "\n",
    "#Plot the first two values (Currently these don't mean much)\n",
    "plt.plot(A2[0].tolist(), A2[1].tolist(), 'x', color='r');\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the \"mean face\" by turning it back into a 2D matrix and using the imshow function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(mean_face.reshape((h, w)), cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989;  <font color=red>** DO THIS:**</font> Pick one of the images.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Start your code here #####\n",
    "image_num =\n",
    "#####End of your code here#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ANSWER##\n",
    "#####Start your code here #####\n",
    "image_num = 1174\n",
    "#####End of your code here#####\n",
    "##ANSWER##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the image as a combination of the mean plus some remainder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remainder = feature_vectors[image_num]-mean_face\n",
    "\n",
    "fig = plt.figure(figsize=(10,20))\n",
    "ax = fig.add_subplot(141)\n",
    "ax.imshow(mean_face.reshape((h, w)), cmap=plt.cm.gray)\n",
    "ax.axis('off')\n",
    "\n",
    "ax = fig.add_subplot(142)\n",
    "ax.imshow(remainder.reshape((h, w)), cmap=plt.cm.gray)\n",
    "ax.axis('off')\n",
    "\n",
    "ax = fig.add_subplot(144)\n",
    "ax.imshow(feature_vectors[image_num].reshape((h, w)), cmap=plt.cm.gray)\n",
    "ax.set_title(categories[class_labels[image_num]])\n",
    "ax.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's just use SciKit Learn instead (it's faster and easier)\n",
    "\n",
    "The above calculation is hard to do when our vectors are really big. \n",
    "There are a lot of things that can slow down the code. \n",
    "Fortunately, the ```PCA``` function in ```sklearn``` is optimized, so that we only do the minimum work we need. \n",
    "Consider the following code which only finds the top ```n_components``` of the PCA solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "#STEPS 1-4\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_components = 4 # This is much less than the original n_features\n",
    "\n",
    "print(\"Extracting the top %d eigenfaces from %d faces\" % (n_components, A.shape[0]))\n",
    "\n",
    "#Set up the pca object with the number of compoents we want to find\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "#Fit the training data to the pca model.\n",
    "pca.fit(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 5\n",
    "pca_faces = pca.transform(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What shape is our transformation matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 6: Visualize the final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_faces[image_num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above values represent a linear combination of the top eigenvectors calculated by PCA for the image you selected. \n",
    "The ```pca.components_``` variable is a list of our top eigenvectors. \n",
    "For example, let's look at the eigenvector corresponding to the larges eigenvalue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "eigen_vector = pca.components_[0]\n",
    "\n",
    "plt.plot(eigen_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the eigenvector is in $R^{1850}$, we can use the same \"trick\" to transform it into an image as we used for the mean image. This is often referred to as the \"Eigenface\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(eigen_vector.reshape((h, w)), cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to visualize the original face as a linear combination of the four eigenvectors (and the mean image):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pca_vec = pca_faces[image_num]\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(171)\n",
    "ax.imshow(mean_face.reshape((h, w)), cmap=plt.cm.gray)\n",
    "ax.set_xlabel('mean face')\n",
    "\n",
    "ax = fig.add_subplot(172)\n",
    "ax.imshow((pca_vec[0]*pca.components_[0]).reshape((h, w)), cmap=plt.cm.gray)\n",
    "ax.set_xlabel(pca_vec[0])\n",
    "\n",
    "ax = fig.add_subplot(173)\n",
    "ax.imshow((pca_vec[1]*pca.components_[1]).reshape((h, w)), cmap=plt.cm.gray)\n",
    "ax.set_xlabel(pca_vec[1])\n",
    "\n",
    "ax = fig.add_subplot(174)\n",
    "ax.imshow((pca_vec[2]*pca.components_[2]).reshape((h, w)), cmap=plt.cm.gray)\n",
    "ax.set_xlabel(pca_vec[2])\n",
    "\n",
    "ax = fig.add_subplot(175)\n",
    "ax.imshow((pca_vec[3]*pca.components_[3]).reshape((h, w)), cmap=plt.cm.gray)\n",
    "ax.set_xlabel(pca_vec[3])\n",
    "\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(177)\n",
    "ax.imshow(feature_vectors[image_num].reshape((h, w)), cmap=plt.cm.gray)\n",
    "ax.set_title(categories[class_labels[image_num]])\n",
    "ax.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We though out a lot of information when we reduced the face representation to four values. \n",
    "The following code reconstructs the face as a linear combination of the eigenvectors and the mean face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Reconstruct the face from the mean_face and a linear combination of the eigenfaces.\n",
    "\n",
    "new_image = mean_face.copy()\n",
    "\n",
    "for i in range(len(pca.components_)):\n",
    "    new_image += pca_vec[i]*pca.components_[i]\n",
    "plt.imshow(new_image.reshape((h, w)), cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Picking the \"best\" Feature Vectors\n",
    "The PCA algorithm finds a transform of the data such that the first component contains the \"most\" information, the second component contains the \"second most\" important information. \n",
    "How much information each component contains is actually included in the ```pca``` object and can be expressed as a ratio from 0 (no information) to 1 (all information). \n",
    "Let's plot these values below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_components = 55 # This is much less than the original n_features\n",
    "\n",
    "print(\"Extracting the top %d eigenfaces from %d faces\" % (n_components, A.shape[0]))\n",
    "\n",
    "#Set up the pca object with the number of compoents we want to find\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "#Fit the training data to the pca model.\n",
    "pca.fit(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Lets plot the variance of the eigen values\n",
    "plt.plot(pca.explained_variance_ratio_, marker=\"o\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to look at this is we can sum up the total ratios and see how much our new set of features represents the \"variance\" in the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_variance = np.sum(pca.explained_variance_ratio_)*100\n",
    "print(\"These %d eigenvectors account for a total of %d percent of the total variance in the original dataset\"\n",
    "      % (n_components, total_variance))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989;  <font color=red>** QUESTION:**</font> How many components would we need to represent 90% of the variance in our original data? (Hint: modify the above code and change ```n_components``` to find a total_variance > 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your answer to the above question here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the result as a linear combination of the 55 eigenvectors and the mean face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image = mean_face.copy()\n",
    "\n",
    "#Transform our face using the latest ```n_components```\n",
    "pca_vec = pca.transform(A[image_num])[0]\n",
    "\n",
    "#Loop over n_components\n",
    "for i in range(len(pca.components_)):\n",
    "    new_image += pca_vec[i]*pca.components_[i]\n",
    "\n",
    "#Reshape and plot new image\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(121)\n",
    "ax.imshow(new_image.reshape((h, w)), cmap=plt.cm.gray)    \n",
    "ax.axis('off');\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.imshow(feature_vectors[image_num].reshape((h, w)), cmap=plt.cm.gray)\n",
    "ax.set_title(categories[class_labels[image_num]])\n",
    "ax.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989;  <font color=red>** DO THIS:**</font> Calculate the difference between the reconstructed image and the original image. \n",
    "Display the difference as an image. \n",
    "Include a ```colorbar``` and select a colormap other than gray to better see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Start your code here #####\n",
    "\n",
    "\n",
    "#####End of your code here#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989;  <font color=red>** DO THIS:**</font> Estimate the error of the reconstruction by calculate the average of the absolute value of the difference at all pixels (Note, this should be a single scalar value):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Start your code here #####\n",
    "error = \n",
    "error\n",
    "#####End of your code here#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Using PCA for Feature Selection in Machine Learning\n",
    "\n",
    "Now let's consider a real world example, where we use PCA to \"select\" the most descriptive components of an image for Machine Learning.\n",
    "\n",
    "**NOTE:** We are using the \"whitening\" option for PCA.  This makes the variance between features equal. For more information see:   http://mccormickml.com/2014/06/03/deep-learning-tutorial-pca-and-whitening/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP A - Split into training and testing set\n",
    "Let's set aside some of the data for training and some of the data for testing.  Scikit learn has a function for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# STEP A - Feature Extraction\n",
    "images = sk_data.images\n",
    "categories = sk_data.target_names\n",
    "class_labels = sk_data.target\n",
    "feature_vectors = sk_data.data\n",
    "feature_vectors.shape\n",
    "\n",
    "full_train_vectors, full_test_vectors, train_labels, test_labels = train_test_split(feature_vectors, class_labels, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989;  <font color=red>** Question:**</font>  In machine learning, why is it important to split the data into a training and testing set? And why do we calculate the principal conponents on the training data only?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your answer to the above question here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step B - Feature Selection using PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a model, we will first reduce the dimensionality of the original picture to a PCA space. This is often called unsupervised feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Outline for Homework 4%%time \n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_components = 55 # This is much less than the original n_features\n",
    "print(\"Extracting the top %d eigenfaces from %d faces\" % (n_components, full_train_vectors.shape[0]))\n",
    "pca = PCA(n_components=n_components, whiten=True)\n",
    "pca.fit(full_train_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pca.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explained Variance for each feature\n",
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.xlabel('Feature Number')\n",
    "plt.ylabel('explained variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick graph to try to show how much variance is in each feature.\n",
    "\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the length of these new vectors are the same size as the original data. Let's plot the gallery of the most significant eigenfaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenfaces = pca.components_.reshape((n_components, h, w))\n",
    "def plot_gallery(images, titles, h, w, n_row=3, n_col=5):\n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "    plt.figure(figsize=(1.7 * n_col, 2.3 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        plt.title(titles[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        \n",
    "eigenface_titles = [\"eigenface %d\" % i for i in range(eigenfaces.shape[0])]\n",
    "plot_gallery(eigenfaces, eigenface_titles, h, w, n_row=2, n_col=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = pca.transform(full_train_vectors)\n",
    "test_vectors = pca.transform(full_test_vectors)\n",
    "\n",
    "print(\"Training set changed from a size of: \", full_train_vectors.shape, ' to: ', train_vectors.shape)\n",
    "print(\"Testing set changed from a size of: \", full_test_vectors.shape, ' to: ', test_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step C: Train an SVM Classifier based on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "###############################################################################\n",
    "# STEP C - Select and train a Classifier using the training dataset.\n",
    "###############################################################################\n",
    "\n",
    "# Train a SVM classification model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "print(\"Fitting the classifier to the training set\")\n",
    "param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
    "clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'), param_grid)\n",
    "clf = clf.fit(train_vectors, train_labels)\n",
    "print(\"Best estimator found by grid search:\")\n",
    "print(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step D: Show the results of the classification on the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP D - Show the results of the classification on the testing dataset\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "pred_labels = clf.predict(test_vectors)\n",
    "\n",
    "print(classification_report(test_labels, pred_labels));\n",
    "print(confusion_matrix(test_labels, pred_labels, labels=range(len(categories))));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_gallery(images, true_titles, pred_titles, h, w, n_row=5, n_col=5):\n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        #plt.title([true_titles[i],pred_titles[i]], size=12)\n",
    "        plt.title(categories[true_titles[i]])\n",
    "        plt.xlabel(categories[pred_titles[i]])\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "\n",
    "plot_gallery(full_test_vectors, test_labels, pred_labels, h,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989;  <font color=red>** DO THIS:**</font>  Adding more eigenfaces should improve recognition.  Can you find a number that will get the precision and recall above 90%?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your answer to the above question here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989;  <font color=red>** QUESTION:**</font>  What happens if you turn off whitening in the above example? Why does this happen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your answer to the above question here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Congratulations, we're almost done!\n",
    "\n",
    "Now, you just need to submit this assignment by uploading it to the course <a href=\"https://d2l.msu.edu/\">Desire2Learn</a> web page for the homework's dropbox (Don't forget to add your names in the first cell).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#169; Copyright 2018,  Michigan State University Board of Trustees"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
